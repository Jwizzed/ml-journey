{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaModel, RobertaTokenizer\nfrom sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import urllib\nurl = \"https://raw.githubusercontent.com/Jwizzed/ml-journey/main/TT.py\"\nurllib.request.urlretrieve(url, \"TT.py\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import TT","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import huggingface_hub\nhuggingface_hub.login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Set the device\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n\n# Create data and send it to the device\nx = torch.rand(size=(3, 4)).to(device)\nx","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get data","metadata":{}},{"cell_type":"code","source":"!kaggle datasets download -d nelgiriyewithana/mcdonalds-store-reviews","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TT.unzip(\"mcdonalds-store-reviews.zip\", delete_original=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"orig_df = pd.read_csv(\"McDonald_s_Reviews.csv\", encoding_errors=\"ignore\", skiprows=lambda i: i % 50 != 0)\ndf = orig_df.copy()\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.category.unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.store_name.unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[[\"rating_count\", \"rating\"]].tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[[\"review\", \"rating\"]]\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocess","metadata":{}},{"cell_type":"code","source":"df.isna().sum(), df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True)\ndf.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop_duplicates(\"review\", keep=\"last\", inplace=True)\ndf.rename(columns={\"rating\":\"labels\", \"review\":\"text\"}, inplace=True)\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.labels.unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"labels\"] = df[\"labels\"].replace({\"1 star\":0, \"2 stars\": 1, \"3 stars\": 2, \"4 stars\": 3, \"5 stars\": 4 })\ndf = df.loc[df['text'].str.contains(r'[^\\x00-\\x7F]+') == False]\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datasets","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 128 \nbatch_size = 32 \n\ndataset = datasets.Dataset.from_pandas(df)\ndataset = dataset.train_test_split(test_size=0.2, seed=42)\ndataset = dataset.remove_columns([\"__index_level_0__\"])\ndataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2label = {0: \"1 star\", 1: \"2 stars\", 2: \"3 stars\", 3: \"4 stars\", 4: \"2 stars\"}\nlabel2id = {\"1 star\":0, \"2 stars\":1, \"3 stars\":2, \"4 stars\":3, \"5 stars\":4}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 256\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nEPOCHS = 5\nLEARNING_RATE = 1e-05\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ReviewData(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.text = dataframe[\"text\"]\n        self.targets = dataframe[\"labels\"]\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long).to(device),\n            'mask': torch.tensor(mask, dtype=torch.long).to(device),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long).to(device),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float).to(device)\n        }\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set = ReviewData(dataset[\"train\"], tokenizer, MAX_LEN)\ntesting_set = ReviewData(dataset[\"test\"], tokenizer, MAX_LEN)\nfor batch in training_set:\n    break\nbatch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Model","metadata":{}},{"cell_type":"code","source":"class RobertaClass(torch.nn.Module):\n    def __init__(self):\n        super(RobertaClass, self).__init__()\n        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n        self.pre_classifier = torch.nn.Linear(768, 768)\n        self.dropout = torch.nn.Dropout(0.3)\n        self.classifier = torch.nn.Linear(768, 5)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        hidden_state = output_1[0]\n        pooler = hidden_state[:, 0]\n        pooler = self.pre_classifier(pooler)\n        pooler = torch.nn.ReLU()(pooler)\n        pooler = self.dropout(pooler)\n        output = self.classifier(pooler)\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RobertaClass()\nmodel.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine Tuning","metadata":{}},{"cell_type":"code","source":"loss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calcuate_accuracy(preds, targets):\n    n_correct = (preds==targets).sum().item()\n    return n_correct","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(epoch):\n    tr_loss = 0 # Accumulated training loss for the current epoch.\n    n_correct = 0 # Number of correct predictions during the epoch.\n    nb_tr_steps = 0 # Total number of training steps within the current epoch.\n    nb_tr_examples = 0 # Total number of training examples processed in the current epoch.\n    model.train()\n    for _,data in tqdm(enumerate(training_loader, 0)):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.long)\n\n        outputs = model(ids, mask, token_type_ids)\n        loss = loss_function(outputs, targets)\n        tr_loss += loss.item()\n        big_val, big_idx = torch.max(outputs.data, dim=1)\n        n_correct += calcuate_accuracy(big_idx, targets)\n\n        nb_tr_steps += 1\n        nb_tr_examples+=targets.size(0)\n        \n        if _%5000==0:\n            loss_step = tr_loss/nb_tr_steps\n            accu_step = (n_correct*100)/nb_tr_examples \n            print(f\"Training Loss per 5000 steps: {loss_step}\")\n            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n\n        optimizer.zero_grad()\n        loss.backward()\n        # # When using GPU\n        optimizer.step()\n\n    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n    epoch_loss = tr_loss/nb_tr_steps\n    epoch_accu = (n_correct*100)/nb_tr_examples\n    print(f\"Training Loss Epoch: {epoch_loss}\")\n    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n    return ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 5\nfor epoch in range(EPOCHS):\n    train(epoch)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid(model, testing_loader):\n    model.eval()\n    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n    with torch.no_grad():\n        for _, data in tqdm(enumerate(testing_loader, 0)):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n            outputs = model(ids, mask, token_type_ids).squeeze()\n            loss = loss_function(outputs, targets)\n            tr_loss += loss.item()\n            big_val, big_idx = torch.max(outputs.data, dim=1)\n            n_correct += calcuate_accuracy(big_idx, targets)\n\n            nb_tr_steps += 1\n            nb_tr_examples+=targets.size(0)\n            \n            if _%5000==0:\n                loss_step = tr_loss/nb_tr_steps\n                accu_step = (n_correct*100)/nb_tr_examples\n                print(f\"Validation Loss per 100 steps: {loss_step}\")\n                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n    epoch_loss = tr_loss/nb_tr_steps\n    epoch_accu = (n_correct*100)/nb_tr_examples\n    print(f\"Validation Loss Epoch: {epoch_loss}\")\n    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n    \n    return epoch_accu\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = valid(model, testing_loader)\nprint(\"Accuracy on test data = %0.2f%%\" % acc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}